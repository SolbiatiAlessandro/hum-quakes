{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Norcia Baseline \u2013 Phase 2: Compute Features per Window\n",
        "\n",
        "This notebook aggregates INSTANCE metadata into per-window seismic features.\n",
        "Inputs: `norcia_events.parquet` (or CSV) and `windows.csv`.\n",
        "\n",
        "Outputs: `norcia_outputs/features.csv`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "EVENT_ID_CANDIDATES = [\"source_id\", \"event_id\", \"event_id_str\"]\n",
        "\n",
        "def load_dataframe(path: Path) -> pd.DataFrame:\n",
        "    if path.suffix == \".parquet\":\n",
        "        return pd.read_parquet(path)\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "def parse_times(df: pd.DataFrame, columns) -> pd.DataFrame:\n",
        "    for column in columns:\n",
        "        if column in df.columns:\n",
        "            df[column] = pd.to_datetime(df[column], utc=True)\n",
        "    return df\n",
        "\n",
        "def pick_event_id_column(df: pd.DataFrame):\n",
        "    for column in EVENT_ID_CANDIDATES:\n",
        "        if column in df.columns:\n",
        "            return column\n",
        "    return None\n",
        "\n",
        "def build_event_frame(window_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    event_id_column = pick_event_id_column(window_df)\n",
        "    event_columns = [\n",
        "        \"source_origin_time\",\n",
        "        \"source_latitude_deg\",\n",
        "        \"source_longitude_deg\",\n",
        "        \"source_magnitude\",\n",
        "        \"source_depth_km\",\n",
        "    ]\n",
        "    available_columns = [col for col in event_columns if col in window_df.columns]\n",
        "\n",
        "    if event_id_column is None:\n",
        "        return window_df.drop_duplicates(subset=available_columns).copy()\n",
        "\n",
        "    grouped = window_df.groupby(event_id_column, as_index=False)\n",
        "    return grouped[available_columns].first()\n",
        "\n",
        "def magnitude_completeness(magnitudes: np.ndarray, bin_size: float = 0.1) -> float:\n",
        "    if magnitudes.size == 0:\n",
        "        return np.nan\n",
        "    min_mag = np.nanmin(magnitudes)\n",
        "    max_mag = np.nanmax(magnitudes)\n",
        "    if not np.isfinite(min_mag) or not np.isfinite(max_mag):\n",
        "        return np.nan\n",
        "    bins = np.arange(min_mag, max_mag + bin_size, bin_size)\n",
        "    counts, edges = np.histogram(magnitudes, bins=bins)\n",
        "    if counts.size == 0:\n",
        "        return np.nan\n",
        "    max_index = int(np.argmax(counts))\n",
        "    return float((edges[max_index] + edges[max_index + 1]) / 2)\n",
        "\n",
        "def b_value_estimate(magnitudes: np.ndarray, mc: float, bin_size: float = 0.1) -> float:\n",
        "    if magnitudes.size < 10 or not np.isfinite(mc):\n",
        "        return np.nan\n",
        "    mags = magnitudes[magnitudes >= mc]\n",
        "    if mags.size < 5:\n",
        "        return np.nan\n",
        "    mean_mag = np.nanmean(mags)\n",
        "    if not np.isfinite(mean_mag) or mean_mag <= (mc - bin_size / 2):\n",
        "        return np.nan\n",
        "    return float(np.log10(np.e) / (mean_mag - (mc - bin_size / 2)))\n",
        "\n",
        "def spatial_spread_km(latitudes: np.ndarray, longitudes: np.ndarray) -> float:\n",
        "    if latitudes.size < 2 or longitudes.size < 2:\n",
        "        return np.nan\n",
        "    lat_std = np.nanstd(latitudes)\n",
        "    lon_std = np.nanstd(longitudes)\n",
        "    if not np.isfinite(lat_std) or not np.isfinite(lon_std):\n",
        "        return np.nan\n",
        "    mean_lat = np.nanmean(latitudes)\n",
        "    lat_km = lat_std * 111.0\n",
        "    lon_km = lon_std * 111.0 * np.cos(np.deg2rad(mean_lat))\n",
        "    return float(np.sqrt(lat_km ** 2 + lon_km ** 2))\n",
        "\n",
        "def compute_window_features(window_df: pd.DataFrame) -> dict:\n",
        "    trace_count = len(window_df)\n",
        "    event_df = build_event_frame(window_df)\n",
        "\n",
        "    magnitudes = event_df.get(\"source_magnitude\", pd.Series(dtype=float)).to_numpy()\n",
        "    depths = event_df.get(\"source_depth_km\", pd.Series(dtype=float)).to_numpy()\n",
        "    latitudes = event_df.get(\"source_latitude_deg\", pd.Series(dtype=float)).to_numpy()\n",
        "    longitudes = event_df.get(\"source_longitude_deg\", pd.Series(dtype=float)).to_numpy()\n",
        "\n",
        "    n_events = len(event_df)\n",
        "    cumulative_moment = np.nansum(np.power(10.0, 1.5 * magnitudes)) if n_events else 0.0\n",
        "\n",
        "    mc = magnitude_completeness(magnitudes)\n",
        "    b_value = b_value_estimate(magnitudes, mc)\n",
        "\n",
        "    return {\n",
        "        \"n_events\": n_events,\n",
        "        \"n_traces\": trace_count,\n",
        "        \"max_magnitude\": np.nanmax(magnitudes) if n_events else np.nan,\n",
        "        \"mean_magnitude\": np.nanmean(magnitudes) if n_events else np.nan,\n",
        "        \"cumulative_moment\": cumulative_moment,\n",
        "        \"mean_depth_km\": np.nanmean(depths) if n_events else np.nan,\n",
        "        \"depth_std_km\": np.nanstd(depths) if n_events else np.nan,\n",
        "        \"spatial_spread_km\": spatial_spread_km(latitudes, longitudes),\n",
        "        \"n_stations\": window_df[\"station_code\"].nunique()\n",
        "        if \"station_code\" in window_df.columns\n",
        "        else np.nan,\n",
        "        \"mean_snr\": window_df[\"trace_E_snr_db\"].mean()\n",
        "        if \"trace_E_snr_db\" in window_df.columns\n",
        "        else np.nan,\n",
        "        \"max_snr\": window_df[\"trace_E_snr_db\"].max()\n",
        "        if \"trace_E_snr_db\" in window_df.columns\n",
        "        else np.nan,\n",
        "        \"mean_pga\": window_df[\"trace_pga_perc\"].mean()\n",
        "        if \"trace_pga_perc\" in window_df.columns\n",
        "        else np.nan,\n",
        "        \"max_pga\": window_df[\"trace_pga_perc\"].max()\n",
        "        if \"trace_pga_perc\" in window_df.columns\n",
        "        else np.nan,\n",
        "        \"mean_rms\": window_df[\"trace_E_rms_counts\"].mean()\n",
        "        if \"trace_E_rms_counts\" in window_df.columns\n",
        "        else np.nan,\n",
        "        \"b_value\": b_value,\n",
        "        \"mc\": mc,\n",
        "    }\n",
        "\n",
        "def compute_features(metadata: pd.DataFrame, windows: pd.DataFrame) -> pd.DataFrame:\n",
        "    features = []\n",
        "    metadata = metadata.copy()\n",
        "    metadata = parse_times(metadata, [\"source_origin_time\"])\n",
        "\n",
        "    for _, window in windows.iterrows():\n",
        "        start = window[\"start_time\"]\n",
        "        end = window[\"end_time\"]\n",
        "        mask = (metadata[\"source_origin_time\"] >= start) & (\n",
        "            metadata[\"source_origin_time\"] < end\n",
        "        )\n",
        "        window_df = metadata.loc[mask]\n",
        "        row = window.to_dict()\n",
        "        row.update(compute_window_features(window_df))\n",
        "        features.append(row)\n",
        "\n",
        "    return pd.DataFrame(features)\n",
        "\n",
        "metadata_path = Path(\"norcia_events.parquet\")\n",
        "windows_path = Path(\"windows.csv\")\n",
        "output_dir = Path(\"norcia_outputs\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "metadata = load_dataframe(metadata_path)\n",
        "windows = load_dataframe(windows_path)\n",
        "windows = parse_times(windows, [\"start_time\", \"end_time\"])\n",
        "\n",
        "features = compute_features(metadata, windows)\n",
        "features_path = output_dir / \"features.csv\"\n",
        "features.to_csv(features_path, index=False)\n",
        "\n",
        "print(f\"\u2713 Features written to {features_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
